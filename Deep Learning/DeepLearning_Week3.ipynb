{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "from utils import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Reshape\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "#from tensorflow.keras.utils.vis_utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_filenames = pickle.load(open('../train_images_filenames.dat','rb'))\n",
    "test_images_filenames = pickle.load(open('../test_images_filenames.dat','rb'))\n",
    "train_images_filenames = ['../..' + n[15:] for n in train_images_filenames]\n",
    "test_images_filenames  = ['../..' + n[15:] for n in test_images_filenames]\n",
    "train_labels = pickle.load(open('../train_labels.dat','rb'))\n",
    "test_labels = pickle.load(open('../test_labels.dat','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = False\n",
    "#user defined variables\n",
    "IMG_SIZE    = 128\n",
    "BATCH_SIZE  = 128\n",
    "\n",
    "#DATASET_DIR = '/home/mcv/datasets/MIT_split'\n",
    "#MODEL_FNAME = '/home/group01/work/my_first_mlp.h5'\n",
    "DATASET_DIR = '../../MIT_split'\n",
    "MODEL_FNAME = '../../my_first_mlp.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATASET_DIR):\n",
    "  print(Color.RED, 'ERROR: dataset directory '+DATASET_DIR+' do not exists!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    print('Building MLP model...\\n')\n",
    "\n",
    "    #Build the Multi Layer Perceptron model\n",
    "    model = Sequential()\n",
    "    model.add(Reshape((IMG_SIZE*IMG_SIZE*3,), input_shape=(IMG_SIZE, IMG_SIZE, 3), name='first'))\n",
    "    model.add(Dense(units=2048, activation='relu',name='second'))\n",
    "    model.add(Dense(units=1024, activation='relu', name='third'))\n",
    "    model.add(Dense(units=512, activation='relu', name='fourth'))\n",
    "    model.add(Dense(units=258, activation='relu', name='fifth'))\n",
    "    model.add(Dense(units=8, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='sgd',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "    #plot_model(model, to_file='modelMLP.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    print('Done!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    if os.path.exists(MODEL_FNAME):\n",
    "        print('WARNING: model file '+MODEL_FNAME+' exists and will be overwritten!\\n')\n",
    "\n",
    "    print('Start training...\\n')\n",
    "\n",
    "    # this is the dataset configuration we will use for training\n",
    "    # only rescaling\n",
    "    train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            horizontal_flip=True)\n",
    "\n",
    "    # this is the dataset configuration we will use for testing:\n",
    "    # only rescaling\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # this is a generator that will read pictures found in\n",
    "    # subfolers of 'data/train', and indefinitely generate\n",
    "    # batches of augmented image data\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "            DATASET_DIR+'/train',  # this is the target directory\n",
    "            target_size=(IMG_SIZE, IMG_SIZE),  # all images will be resized to IMG_SIZExIMG_SIZE\n",
    "            batch_size=BATCH_SIZE,\n",
    "            classes = ['coast','forest','highway','inside_city','mountain','Opencountry','street','tallbuilding'],\n",
    "            class_mode='categorical')  # since we use binary_crossentropy loss, we need categorical labels\n",
    "\n",
    "    # this is a similar generator, for validation data\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "            DATASET_DIR+'/test',\n",
    "            target_size=(IMG_SIZE, IMG_SIZE),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            classes = ['coast','forest','highway','inside_city','mountain','Opencountry','street','tallbuilding'],\n",
    "            class_mode='categorical')\n",
    "\n",
    "    history = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=1881 // BATCH_SIZE,\n",
    "            epochs=150,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=807 // BATCH_SIZE)\n",
    "\n",
    "    print('Done!\\n')\n",
    "    print('Saving the model into '+MODEL_FNAME+' \\n')\n",
    "    model.save_weights(MODEL_FNAME)  # always save your weights after training or during training\n",
    "    print('Done!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    # summarize history for accuracy\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.savefig('accuracy.jpg')\n",
    "    #plt.close()\n",
    "    # summarize history for loss\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.savefig('loss.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained models against previous techniques\n",
    "\n",
    "___\n",
    "\n",
    "Here we will compare the performances of an end-to-end MLP against the performances of a SVM classifier trained on the features learned by this same MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End to end Multi-Layer Perceptron\n",
    "\n",
    "After training a Neural Network with the code above, we will benchmark the performances of this model on our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the Multi Layer Perceptron model\n",
    "model = Sequential()\n",
    "model.add(Reshape((IMG_SIZE*IMG_SIZE*3,), input_shape=(IMG_SIZE, IMG_SIZE, 3), name='first'))\n",
    "model.add(Dense(units=2048, activation='relu',name='second'))\n",
    "model.add(Dense(units=1024, activation='relu', name='third'))\n",
    "model.add(Dense(units=512, activation='relu', name='fourth'))\n",
    "model.add(Dense(units=258, activation='relu', name='fifth'))\n",
    "model.add(Dense(units=8, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Model Weight\n",
    "MODEL_LOAD_NAME = '../../my_first_mlp_128x128_150epoch.h5'\n",
    "model.load_weights(MODEL_LOAD_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mDone!images: 807 / 807\n",
      "\u001b[0m\u001b[32mTest Acc. = 0.6468401486988847\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "directory = DATASET_DIR+'/test'\n",
    "classes = {'coast':0,'forest':1,'highway':2,'inside_city':3,'mountain':4,'Opencountry':5,'street':6,'tallbuilding':7}\n",
    "correct = 0\n",
    "total   = 807\n",
    "count   = 0\n",
    "\n",
    "for imname, label in zip(test_images_filenames, test_labels):\n",
    "    im = np.asarray(Image.open(os.path.join(imname)))\n",
    "    im = np.expand_dims(cv2.resize(im, (IMG_SIZE, IMG_SIZE)), axis=0)\n",
    "    cls = classes[label]\n",
    "    out = model.predict(im)\n",
    "    predicted_cls = np.argmax( out )\n",
    "    if predicted_cls == cls:\n",
    "        correct+=1\n",
    "    count += 1\n",
    "    print('Evaluated images: '+str(count)+' / '+str(total), end='\\r')\n",
    "    \n",
    "colorprint(Color.BLUE, 'Done!\\n')\n",
    "colorprint(Color.GREEN, 'Test Acc. = '+str(correct/total)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classifier with learnt features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_layer = Model(inputs=model.input, outputs=model.get_layer('fourth').output)\n",
    "layer_preds = []\n",
    "layer_classes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"train_layer_preds.pkl\"):\n",
    "\n",
    "    #We will start by creating features training sets\n",
    "    count, total = 0, 1881\n",
    "\n",
    "    for imname, label in zip(train_images_filenames, train_labels):\n",
    "        im = np.asarray(Image.open(os.path.join(imname)))\n",
    "        im = np.expand_dims(cv2.resize(im, (IMG_SIZE, IMG_SIZE)), axis=0)\n",
    "        cls = classes[label]\n",
    "\n",
    "        layer_preds.append(model_layer.predict(im))\n",
    "        layer_classes.append(cls)\n",
    "    \n",
    "        count += 1\n",
    "        print('Added images: '+ str(count)+' / '+str(total), end='\\r')\n",
    "\n",
    "    X_train = np.vstack(layer_preds)\n",
    "    y_train = np.array(layer_classes)\n",
    "    \n",
    "    with open(\"train_layer_preds.pkl\", \"wb\") as f: \n",
    "        pickle.dump(layer_preds, f)\n",
    "    with open(\"train_layer_preds_classes.pkl\", \"wb\") as f:\n",
    "        pickle.dump(layer_classes, f)\n",
    "\n",
    "else:\n",
    "    X_train = pickle.load(open(\"train_layer_preds.pkl\",\"rb\"))\n",
    "    y_train = pickle.load(open(\"train_layer_preds_classes.pkl\",\"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(\"test_layer_preds.pkl\"):\n",
    "\n",
    "    layer_preds = []\n",
    "    layer_classes = []\n",
    "\n",
    "    #We will start by creating features training sets\n",
    "    count, total = 0, 807\n",
    "\n",
    "    for imname, label in zip(test_images_filenames, test_labels):\n",
    "        im = np.asarray(Image.open(os.path.join(imname)))\n",
    "        im = np.expand_dims(cv2.resize(im, (IMG_SIZE, IMG_SIZE)), axis=0)\n",
    "        cls = classes[label]\n",
    "\n",
    "        layer_preds.append(model_layer.predict(im))\n",
    "        layer_classes.append(cls)\n",
    "    \n",
    "        count += 1\n",
    "        print('Added images: '+ str(count)+' / '+str(total), end='\\r')\n",
    "\n",
    "    X_test = np.vstack(layer_preds)\n",
    "    y_test = np.array(layer_classes)\n",
    "    \n",
    "    with open(\"test_layer_preds.pkl\", \"wb\") as f: \n",
    "        pickle.dump(layer_preds, f)\n",
    "    with open(\"test_layer_preds_classes.pkl\", \"wb\") as f:\n",
    "        pickle.dump(layer_classes, f)\n",
    "\n",
    "else:\n",
    "    X_test = pickle.load(open(\"test_layer_preds.pkl\",\"rb\"))\n",
    "    y_test = pickle.load(open(\"test_layer_preds_classes.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training set : 0.965\n",
      "Accuracy on the test set : 0.674\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Define SVM Model and scaler\n",
    "svm_model = svm.SVC()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit_transform(X_train)\n",
    "scaler.transform(X_test)\n",
    "\n",
    "#Fit the SVM model to the training data\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_train_preds = svm_model.predict(X_train)\n",
    "svm_test_preds = svm_model.predict(X_test)\n",
    "\n",
    "#Get accuracy score\n",
    "print(f\"Accuracy on the training set : {round(accuracy_score(y_train, svm_train_preds),3)}\")\n",
    "print(f\"Accuracy on the test set : {round(accuracy_score(y_test, svm_test_preds),3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e761f16af4fd4b55d5bcd3eb004ed95f62065dc2002a55b73b32bd3196c9eb9d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('Tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
