{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M3_Week3_Model_Experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1TPt9RAjKuqT5krd9JhLc8YEIZe-J5e7-",
      "authorship_tag": "ABX9TyOi1SKt0yXltCDwEsHZjPe3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myazann/M3-Machine-Learning/blob/main/M3_Week3_Model_Experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/utils.py .\n",
        "!cp drive/MyDrive/mlp_MIT_8_scene.py .\n",
        "!cp drive/MyDrive/patch_based_mlp_MIT_8_scene.py .\n",
        "\n",
        "!cp -a drive/MyDrive/MIT_split .\n",
        "!mkdir work"
      ],
      "metadata": {
        "id": "knisi30MO3yV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9xK4293dIWKr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "from utils import *\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Flatten, Dense, Reshape\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "work_dir = \"work/\" + str(time.time())\n",
        "os.mkdir(work_dir)\n",
        "\n",
        "IMG_SIZE = 64\n",
        "BATCH_SIZE  = 16\n",
        "#DATASET_DIR = '/home/mcv/datasets/MIT_split'\n",
        "DATASET_DIR = 'MIT_split'\n",
        "#MODEL_FNAME = '/home/group06/work/my_first_mlp.h5'\n",
        "MODEL_FNAME = work_dir + '/model.h5'\n",
        "\n",
        "if not os.path.exists(DATASET_DIR):\n",
        "    print(Color.RED, 'ERROR: dataset directory ' + DATASET_DIR + ' do not exists!\\n')\n",
        "\n",
        "print('Building MLP model...\\n')\n",
        "\n",
        "\n",
        "#Build the Multi Layer Perceptron model\n",
        "model = Sequential()\n",
        "model.add(Reshape((IMG_SIZE*IMG_SIZE*3,), input_shape=(IMG_SIZE, IMG_SIZE, 3), name='first'))\n",
        "model.add(Dense(units=2048, activation='leaky_relu',name='second'))\n",
        "model.add(Dense(units=1024, activation='leaky_relu', name='third'))\n",
        "model.add(Dense(units=512, activation='leaky_relu', name='fourth'))\n",
        "model.add(Dense(units=8, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.SGD(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())\n",
        "plot_model(model, to_file=work_dir + '/model.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "print('Done!\\n')\n",
        "\n",
        "if os.path.exists(MODEL_FNAME):\n",
        "    print('WARNING: model file ' + MODEL_FNAME + ' exists and will be overwritten!\\n')\n",
        "\n",
        "print('Start training...\\n')\n",
        "\n",
        "# this is the dataset configuration we will use for training\n",
        "# only rescaling\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "# this is the dataset configuration we will use for testing:\n",
        "# only rescaling\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# this is a generator that will read pictures found in\n",
        "# subfolers of 'data/train', and indefinitely generate\n",
        "# batches of augmented image data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        DATASET_DIR+'/train',  # this is the target directory\n",
        "        target_size=(IMG_SIZE, IMG_SIZE),  # all images will be resized to IMG_SIZExIMG_SIZE\n",
        "        batch_size=BATCH_SIZE,\n",
        "        classes = ['coast','forest','highway','inside_city','mountain','Opencountry','street','tallbuilding'],\n",
        "        class_mode='categorical')  # since we use binary_crossentropy loss, we need categorical labels\n",
        "\n",
        "# this is a similar generator, for validation data\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        DATASET_DIR+'/test',\n",
        "        target_size=(IMG_SIZE, IMG_SIZE),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        classes = ['coast','forest','highway','inside_city','mountain','Opencountry','street','tallbuilding'],\n",
        "        class_mode='categorical')\n",
        "\n",
        "history = model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=1881 // BATCH_SIZE,\n",
        "        epochs=50,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=807 // BATCH_SIZE)\n",
        "\n",
        "print('Done!\\n')\n",
        "print('Saving the model into ' + MODEL_FNAME + ' \\n')\n",
        "model.save(MODEL_FNAME)  # always save your weights after training or during training\n",
        "print('Done!\\n')\n",
        "\n",
        "  # summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.savefig(work_dir + '/accuracy.jpg')\n",
        "plt.close()\n",
        "  # summarize history for loss\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.savefig(work_dir + '/loss.jpg')\n",
        "\n"
      ],
      "metadata": {
        "id": "RhWPO9MLOBO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "second_layer = Model(inputs=model.input, outputs=model.get_layer('second').output)\n",
        "third_layer = Model(inputs=model.input, outputs=model.get_layer('third').output)\n",
        "fourth_layer = Model(inputs=model.input, outputs=model.get_layer('fourth').output)"
      ],
      "metadata": {
        "id": "Jv0stpuR7w_r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(DATASET_DIR + \"/test_images_filenames.dat\") as test_images_filenames:\n",
        "  test_images_filenames = test_images_filenames.readlines()\n",
        "  test_files = [k[k.find(\"MIT_split\"):].strip()[:-1] for k in test_images_filenames if \"MIT_split\" in k]\n",
        "\n",
        "second_layer_preds = []\n",
        "third_layer_preds = []\n",
        "fourth_layer_preds = []\n",
        "\n",
        "for img in test_files:\n",
        "  x = np.asarray(Image.open(img))\n",
        "  x = np.expand_dims(cv2.resize(x, (IMG_SIZE, IMG_SIZE)), axis=0)/255.0\n",
        "\n",
        "  second_layer_preds.append(second_layer.predict(x))\n",
        "  third_layer_preds.append(third_layer.predict(x))\n",
        "  fourth_layer_preds.append(fourth_layer.predict(x))\n",
        "  \n",
        "second_layer_preds = np.vstack(second_layer_preds)\n",
        "third_layer_preds = np.vstack(third_layer_preds)\n",
        "fourth_layer_preds = np.vstack(fourth_layer_preds)"
      ],
      "metadata": {
        "id": "hC0haVV678o1"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "HBcVRq03BncL"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"second_layer_preds.pkl\", \"wb\") as f: \n",
        "  pickle.dump(second_layer_preds, f)\n",
        "\n",
        "with open(\"third_layer_preds.pkl\", \"wb\") as f: \n",
        "  pickle.dump(third_layer_preds, f)\n",
        "\n",
        "with open(\"fourth_layer_preds.pkl\", \"wb\") as f: \n",
        "  pickle.dump(fourth_layer_preds, f)"
      ],
      "metadata": {
        "id": "IxfLJTB8BhUR"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(MODEL_FNAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgTnu4V2E9-G",
        "outputId": "de08ec14-1bab-4102-86fe-117c2b5084b0"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model(\"/content/work/1642622595.826117/model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvKRVyaoFUHn",
        "outputId": "52f83039-71fe-4f34-be46-6f835d8d9a4a"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r work.zip work > /dev/null"
      ],
      "metadata": {
        "id": "OrGRnaGhCLQ3"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5ZmILkNzGN9w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
